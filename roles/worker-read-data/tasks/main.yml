- name: Create a nfs volume for worker data
  docker_volume:
    name: worker_data_nfs
    driver: local
    driver_options:
      type: nfs
      device: :/shared
      o: addr=agent-nfs.{{ consul_node_domain }}

- name: Get amount of cpu cores and calculate amount of workers to start
  shell: grep -c ^processor /proc/cpuinfo
  register: cpu_cores

- set_fact:
    worker_instance_count: "{{ cpu_cores.stdout | int }}"

- name: Ensure read data worker containers are started
  docker_container:
    name: "worker-read-data-{{ item }}"
    image: inowas/pymodelling
    state: started
    restart: yes
    volumes:
      - worker_data_nfs:/data
    dns_servers:
    - "{{ consul_agent_ip }}"
    env:
      RABBITMQ_HOST: "{{ rabbitmq_host }}"
      RABBITMQ_PORT: "{{ rabbitmq_port }}"
      RABBITMQ_VIRTUAL_HOST: "{{ rabbitmq_vhost }}"
      RABBITMQ_USER: "{{ rabbitmq_user }}"
      RABBITMQ_PASSWORD: "{{ rabbitmq_password }}"
      READ_DATA_QUEUE: rpc_flopy_read_data_queue
    command: python -u /pymodelling/inowas.flopy.read_data.rpc.server.py /data
  with_sequence: count={{ worker_instance_count }}
